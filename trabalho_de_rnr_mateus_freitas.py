# -*- coding: utf-8 -*-
"""Trabalho de RnR - Mateus Freitas

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oj2AmDHTpS4zZhT5WkCed9urOf9tJEIX

## Geração Automática de Texto com LSTMs

# Etapa 1: Importação das bibliotecas
"""

# Imports
import pandas
import numpy
import sys
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from keras.layers import LSTM
from keras.callbacks import ModelCheckpoint
from keras.utils import np_utils

"""# Etapa 2: Carregamento e exploração da base de dados"""

#carregando os dados
from google.colab import drive
drive.mount('/content/drive')

dataframe = pandas.read_csv('/content/drive/MyDrive/Colab Notebooks/elon_musk_tweets.csv')

"""# Etapa 3: Tratamento da base de dados"""

dataframe

#Pegando a coluna dos tweets
dataframe2 = dataframe['text'].tolist()

#transformando todas as letras em minúsculas
tweets = ",".join(dataframe2)
tweets = tweets.lower()
tweets

chars = sorted(list(set(tweets)))

chars

char_to_int = dict((c, i) for i, c in enumerate(chars))

char_to_int

#tirando os caracteres estranhos
contador = 55
while contador < 93:
    tweets = tweets.replace(chars[contador], ' ')
    contador += 1
    if contador == 91:
      contador += 1
tweets

chars2 = sorted(list(set(tweets)))

chars2

char_to_int = dict((c, i) for i, c in enumerate(chars2))

char_to_int

n_chars = len(tweets)
n_vocab = len(chars2)
print ("Total Characters: ", n_chars)
print ("Total Vocab: ", n_vocab)

"""#Etapa 4: Split em treino e teste"""

# À medida que dividimos os tweets em sequências, convertemos os caracteres em números inteiros usando nossa
# tabela de pesquisa que preparamos anteriormente.
seq_length = 100
dataX = []
dataY = []

for i in range(0, n_chars - seq_length, 1):
    seq_in = tweets[i:i + seq_length]
    seq_out = tweets[i + seq_length]
    dataX.append([char_to_int[char] for char in seq_in])
    dataY.append(char_to_int[seq_out])
n_patterns = len(dataX)
print ("Total de Padrões: ", n_patterns)

dataX

# Reshape de X para [samples, time steps, features]
X = numpy.reshape(dataX, (n_patterns, seq_length, 1))

# Normalização
X = X / float(n_vocab)

# One-Hot Encoding da variável de saída
y = np_utils.to_categorical(dataY)

y

"""# Etapa 6: Construção e treinamento do modelo
Modelo LSTM com duas camadas de Dropout com 20%
O tempo de treinamento é bem longo
"""

model = Sequential()
model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(256))
model.add(Dropout(0.2))
model.add(Dense(y.shape[1], activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam')

# Define o checkpoint
filepath = "weights-improvement-{epoch:02d}-{loss:.4f}.hdf5"
checkpoint = ModelCheckpoint(filepath, monitor = 'loss', verbose = 1, save_best_only = True, mode = 'min')
callbacks_list = [checkpoint]

"""Fit do modelo"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# model.fit(X, y, epochs = 100, batch_size = 64, callbacks = callbacks_list)

"""Etapa 7: Geração do Texto"""

# Carrega os melhores pesos da rede e compila o modelo
filename = "/content/weights-improvement-96-0.3919.hdf5"
model.load_weights(filename)
model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')

int_to_char = dict((i, c) for i, c in enumerate(chars2))

# Obtém um random seed
start = numpy.random.randint(0, len(dataX)-1)

# Inicia a geração de texto de um ponto qualquer, definido pelo random seed "start"
pattern = dataX[start]
print ("\"", ''.join([int_to_char[value] for value in pattern]), "\"")

# Gerando caracteres
for i in range(200):
    x = numpy.reshape(pattern, (1, len(pattern), 1))
    x = x / float(n_vocab)
    prediction = model.predict(x, verbose=0)
    index = numpy.argmax(prediction)
    result = int_to_char[index]
    seq_in = [int_to_char[value] for value in pattern]
    sys.stdout.write(result)
    pattern.append(index)
    pattern = pattern[1:len(pattern)]
print ("\nConcluído.")